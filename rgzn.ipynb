{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d175c38635d8b3f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model_incrementally(file_path, existing_model, params=None, num_rounds=100):\n",
    "\n",
    "       df1 = pd.read_csv(file_path)\n",
    "       \n",
    "       # Preprocess data\n",
    "       df1.iloc[:, 2] = df1.iloc[:, 2].str.extract('(\\d+)').astype(int)\n",
    "       df1.iloc[:,-9:] = df1.iloc[:,-9:].applymap(lambda x: 1 if x != 0 else 0)\n",
    "       df1 = df1.astype(int)   \n",
    "       error_df = pd.DataFrame()\n",
    "       fault_columns = [col for col in df1.columns if '故障' in col]\n",
    "   \n",
    "       for col in fault_columns:\n",
    "           error_df[f'{col}_start'] = (df1[col] != 0) & (df1[col].shift(1) == 0)\n",
    "           error_df[f'{col}_duration'] = df1.groupby((df1[col] == 0).cumsum())[col].transform('count') * error_df[f'{col}_start']\n",
    "           duration_temp = pd.Series(index=error_df.index, dtype='float64')\n",
    "           \n",
    "           for i in error_df.index[error_df[f'{col}_start']]:\n",
    "               duration_temp.iloc[i:i + error_df.at[i, f'{col}_duration']] = error_df.at[i, f'{col}_duration']\n",
    "           \n",
    "           error_df[f'{col}_duration'] = duration_temp.fillna(0).astype(int)\n",
    "   \n",
    "       for col in fault_columns:\n",
    "           df1[f'{col}_duration'] = error_df[f'{col}_duration']\n",
    "   \n",
    "       X = df1.iloc[:, :-18].values\n",
    "       y = df1.iloc[:, -18:-9].values\n",
    "       \n",
    "       X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "       y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "   \n",
    "   #     oversampled_X_list = []\n",
    "   #     oversampled_y_list = []\n",
    "   \n",
    "   #     for i in range(y.shape[1]):\n",
    "   #         minority_class = y[:, i] == 1\n",
    "   #         majority_class = ~minority_class\n",
    "   #         target_minority_samples = int(X[majority_class].shape[0] / 3)\n",
    "           \n",
    "  #         X_minority_oversampled, y_minority_oversampled = resample(\n",
    "   #             X[minority_class], y[minority_class][:, i],\n",
    "  #             replace=True,\n",
    "  #             n_samples=target_minority_samples,\n",
    "   #             random_state=42\n",
    "   #         )\n",
    "           \n",
    "   #         X_oversampled = np.vstack((X[majority_class], X_minority_oversampled))\n",
    "   #         y_oversampled = np.hstack((y[majority_class][:, i], y_minority_oversampled))\n",
    "          \n",
    "  #         oversampled_X_list.append(X_oversampled)\n",
    "   #         oversampled_y_list.append(y_oversampled)\n",
    "   \n",
    "   #     oversampled_X = np.concatenate(oversampled_X_list, axis=0)\n",
    "   #     oversampled_y = np.concatenate(oversampled_y_list, axis=0)\n",
    "       \n",
    "       dtrain = xgb.DMatrix(X_tensor, label=y_tensor)\n",
    "   \n",
    "       # Set default parameters if none provided\n",
    "       if params is None:\n",
    "           params = {\n",
    "               'max_depth': 3,\n",
    "               'eta': 0.3,\n",
    "               'objective': 'binary:logistic',\n",
    "               'eval_metric': 'auc'\n",
    "           }\n",
    "   \n",
    "       # Continue training the model\n",
    "       bst = xgb.train(params, dtrain, num_rounds, xgb_model=existing_model)\n",
    "   \n",
    "       return bst\n",
    "   #%% \n",
    "initial_model = None  # 如果有现有模型，将其路径或实例替换这里\n",
    "trained_model = train_model_incrementally(\"F:\\A\\content\\contenta\\A.date\\A题-全部数据 2\\A题-全部数据\\附件1\\M101.csv\", initial_model)\n",
    "\n",
    "file_path_template = \"F:\\A\\content\\contenta\\A.date\\A题-全部数据 2\\A题-全部数据\\附件1\\M{}.csv\"\n",
    "for i in tqdm(range(102,111)):\n",
    "    filename = file_path_template.format(i)\n",
    "    trained_model = train_model_incrementally(filename, trained_model)\n",
    "   #%% \n",
    "   # 模型\n",
    "model_path_json = \"xgboost.json\"  # 指定路径并使用 .json 扩展名\n",
    "   \n",
    "trained_model.save_model(model_path_json)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18690ef617b7f3d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95a29f97143dbfa6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path_jsonn = './xgboost.json'\n",
    "\n",
    "# 从 JSON 加载模型\n",
    "loaded_bst_jsonn = xgb.Booster()\n",
    "loaded_bst_jsonn.load_model(model_path_jsonn)\n",
    "loaded_bst_jsonn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6baa39f6fd88f0e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_process(file_path):\n",
    "    # Load data\n",
    "    df1 = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df1.iloc[:, 2] = df1.iloc[:, 2].str.extract('(\\d+)').astype(int)\n",
    "    df1.iloc[:,-9:] = df1.iloc[:,-9:].applymap(lambda x: 1 if x != 0 else 0)\n",
    "    df1 = df1.astype(int)\n",
    "\n",
    "    error_df = pd.DataFrame()\n",
    "    fault_columns = [colm for colm in df1.columns if '故障' in colm]\n",
    "\n",
    "    for colm in fault_columns:\n",
    "        error_df[f'{colm}_start'] = (df1[colm] != 0) & (df1[colm].shift(1) == 0)\n",
    "        error_df[f'{colm}_duration'] = df1.groupby((df1[colm] == 0).cumsum())[colm].transform('count') * error_df[f'{colm}_start']\n",
    "        duration_temp = pd.Series(index=error_df.index, dtype='float64')\n",
    "        \n",
    "        for i in error_df.index[error_df[f'{colm}_start']]:\n",
    "            duration_temp.iloc[i:i + error_df.at[i, f'{colm}_duration']] = error_df.at[i, f'{colm}_duration']\n",
    "        \n",
    "        error_df[f'{colm}_duration'] = duration_temp.fillna(0).astype(int)\n",
    "\n",
    "    for colm in fault_columns:\n",
    "        df1[f'{colm}_duration'] = error_df[f'{colm}_duration']\n",
    "\n",
    "    X = df1.iloc[:, :-18].values\n",
    "    y = df1.iloc[:, -18:-9].values\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    return X_tensor, y_tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c995e7a057c6840"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, Y = data_process('F:\\A\\content\\contenta\\A.date\\A题-全部数据 2\\A题-全部数据\\附件1/M101.csv')\n",
    "X.shape, Y.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "671dea4edd549613"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 在测试集上进行预测\n",
    "dtest = xgb.DMatrix(X, label=Y)\n",
    "\n",
    "preds = loaded_bst_jsonn.predict(dtest)\n",
    "predictions = np.around(preds).astype(int)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(Y, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100.0}%\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4334758b57545f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, Y = data_process('F:\\A\\content\\contenta\\A.date\\A题-全部数据 2\\A题-全部数据\\附件2\\数据.csv')\n",
    "X.shape, Y.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c95aa3738091152"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 在测试集上进行预测\n",
    "dtest0 = xgb.DMatrix(X, label=Y)\n",
    "\n",
    "preds0 = loaded_bst_jsonn.predict(dtest)\n",
    "predictions = np.around(preds).astype(int)\n",
    "# 假设outputs是你的神经网络输出的Tensor\n",
    "# 转换为NumPy数组\n",
    "outputs_np = predictions\n",
    "\n",
    "# 绘制直方图\n",
    "plt.hist(outputs_np, bins=9, alpha=0.8, label='Predicted Probabilities')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Predicted Probabilities')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 或者绘制折线图\n",
    "plt.plot(outputs_np, label='Predicted Probabilities')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.title('Predicted Probabilities for Each Sample')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463fa952d5856184"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre = np.load('array.npy')\n",
    "pre"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aef8def6b2170128"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('F:\\A\\content\\contenta\\A.date\\A题-全部数据 2\\A题-全部数据\\附件2\\数据.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "608a9ce8b45c73e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 将列表转换为 Pandas Series\n",
    "series = pd.Series(pre[:,0])\n",
    "\n",
    "# 计算故障开始的位置：1 出现的地方，并且前一个元素是 0\n",
    "start = series[(series == 1) & (series.shift(1) != 1)].index\n",
    "\n",
    "# 计算故障结束的位置：1 后面跟着 0 的位置\n",
    "end = series[(series == 1) & (series.shift(-1) != 1)].index\n",
    "\n",
    "# 计算每次故障的持续时间\n",
    "durations = end - start + 1\n",
    "\n",
    "# 创建一个 DataFrame 来保存故障的开始时间和持续时间\n",
    "df1 = pd.DataFrame({\n",
    "    'Fault Start': start,\n",
    "    '持续时长/秒': durations\n",
    "})\n",
    "\n",
    "# 显示 DataFrame\n",
    "df1 = df1[df1['持续时长/秒']>1]\n",
    "df1.set_index('Fault Start', inplace=True)\n",
    "result_df = df1.merge(df.iloc[:,:2], left_index=True, right_index=True, how='left')\n",
    "df_reset = result_df.reset_index(drop=True)\n",
    "df_reset['序号'] = df_reset.index\n",
    "df_reset = df_reset.iloc[:,[3,1,2,0]]\n",
    "df_reset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e57126c96236414"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# 遍历 prediction 的每一列\n",
    "for i in range(pre.shape[1]):\n",
    "    # 将列转换为 Pandas Series\n",
    "    series = pd.Series(pre[:, i])\n",
    "    \n",
    "    # 计算故障开始的位置：1 出现的地方，并且前一个元素是 0\n",
    "    start = series[(series == 1) & (series.shift(1) != 1)].index\n",
    "\n",
    "    # 计算故障结束的位置：1 后面跟着 0 的位置\n",
    "    end = series[(series == 1) & (series.shift(-1) != 1)].index\n",
    "\n",
    "    # 计算每次故障的持续时间\n",
    "    duration = end - start + 1\n",
    "\n",
    "    # 创建一个 DataFrame 来保存故障的开始时间和持续时间\n",
    "    df1 = pd.DataFrame({\n",
    "        'Fault Start': start,\n",
    "        '持续时长/秒': duration\n",
    "    })\n",
    "\n",
    "    # 显示 DataFrame\n",
    "    df1 = df1[df1['持续时长/秒']>1]\n",
    "    df1.set_index('Fault Start', inplace=True)\n",
    "    df1 = df1.merge(df.iloc[:,:2], left_index=True, right_index=True, how='left')\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "#     df1['序号'] = df1.index\n",
    "    df1 = df1.iloc[:,[1,2,0]]\n",
    "    \n",
    "    # 添加到列表\n",
    "    dfs.append(df1)\n",
    "\n",
    "# 合并所有 DataFrame，横向并排\n",
    "result_df = pd.concat(dfs, axis=1)\n",
    "\n",
    "# 显示结果\n",
    "result_df.to_csv('results2.csv')\n",
    "result_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45179489f9739f01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21df6b7d6006cd6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
